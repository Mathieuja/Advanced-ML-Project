{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35ab303",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yfinance\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "import copy\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388f60a",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "html = requests.get(url, headers=headers)\n",
    "SPY_tickers = pd.read_html(html.text)[0]['Symbol'].tolist()\n",
    "SPY_tickers.append(\"^GSPC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf24aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(SPY_tickers, start=\"2000-01-01\", end=\"2025-11-25\", auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03543df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"data/SPY_data.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e33481",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"data/SPY_data.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stocks = data['Close'].columns.unique()[(data['Close'].isna().mean()<0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_close = np.log(data['Close'])\n",
    "log_open  = np.log(data['Open'])\n",
    "log_high  = np.log(data['High'])\n",
    "log_low   = np.log(data['Low'])\n",
    "\n",
    "stocks = selected_stocks[:-1]\n",
    "index = '^GSPC'\n",
    "\n",
    "# --- returns ---\n",
    "close_diff = log_close[stocks].diff()\n",
    "index_ret  = log_close[index].diff()\n",
    "\n",
    "df = close_diff.mul(1e4).stack().rename(\"close_1d_ret\").reset_index()\n",
    "\n",
    "df[\"close_1d_ret_hedged\"] = (\n",
    "    (close_diff.sub(index_ret, axis=0) * 1e4)\n",
    "    .stack()\n",
    "    .values\n",
    ")\n",
    "\n",
    "# --- open-close ---\n",
    "open_close = (log_close[stocks] - log_open[stocks]) * 1e4\n",
    "open_close_idx = (log_close[index] - log_open[index]) * 1e4\n",
    "\n",
    "df = df.merge(\n",
    "    open_close.stack().rename(\"open_close_ret\").reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    (open_close.sub(open_close_idx, axis=0))\n",
    "        .stack()\n",
    "        .rename(\"open_close_ret_hedged\")\n",
    "        .reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "# --- close-open ---\n",
    "\n",
    "close_open = (log_open[stocks] - log_close[stocks].shift()) * 1e4\n",
    "close_open_idx = (log_open[index] - log_close[index].shift()) * 1e4\n",
    "\n",
    "df = df.merge(\n",
    "    close_open.stack().rename(\"close_open_ret\").reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    (close_open.sub(close_open_idx, axis=0))\n",
    "        .stack()\n",
    "        .rename(\"close_open_ret_hedged\")\n",
    "        .reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "# --- high-low ---\n",
    "high_low = (log_high[stocks] - log_low[stocks]) * 1e4\n",
    "high_low_idx = (log_high[index] - log_low[index]) * 1e4\n",
    "\n",
    "df = df.merge(\n",
    "    high_low.stack().rename(\"high_low_ret\").reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    (high_low.sub(high_low_idx, axis=0))\n",
    "        .stack()\n",
    "        .rename(\"high_low_ret_hedged\")\n",
    "        .reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "# --- volumes ---\n",
    "volume = data['Volume'][stocks]\n",
    "dollar_volume = volume * (data['Open'][stocks] + data['Close'][stocks])/2\n",
    "dolar_volume_share = volume.div(volume.sum(axis=1), axis=0)\n",
    "\n",
    "df = df.merge(\n",
    "    volume.stack().rename(\"volume\").reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    dollar_volume.stack().rename(\"dollar_volume\").reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    dolar_volume_share.stack().rename(\"share_dollar_volume\").reset_index(),\n",
    "    on=[\"Date\", \"Ticker\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_ratio'] = df['open_close_ret'] / (df['high_low_ret']+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce486f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['turnover_proxy'] = df['dollar_volume'] / df.groupby('Date')['dollar_volume'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f36e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['close_1d_ret_lag1'] = df.groupby('Ticker')['close_1d_ret'].shift()\n",
    "df['close_1d_ret_hedged_lag1'] = df.groupby('Ticker')['close_1d_ret_hedged'].shift()\n",
    "df['open_close_ret'] = df.groupby('Ticker')['open_close_ret'].shift()\n",
    "df['open_close_ret_hedged'] = df.groupby('Ticker')['open_close_ret_hedged'].shift()\n",
    "df['close_open_ret'] = df.groupby('Ticker')['close_open_ret'].shift()\n",
    "df['close_open_ret_hedged'] = df.groupby('Ticker')['close_open_ret_hedged'].shift()\n",
    "df['high_low_ret'] = df.groupby('Ticker')['high_low_ret'].shift()\n",
    "df['high_low_ret_hedged'] = df.groupby('Ticker')['high_low_ret_hedged'].shift()\n",
    "df['share_dollar_volume'] = df.groupby('Ticker')['share_dollar_volume'].shift()\n",
    "df['body_ratio'] = df.groupby('Ticker')['body_ratio'].shift()\n",
    "df['turnover_proxy'] = df.groupby('Ticker')['turnover_proxy'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_features = ['close_1d_ret_lag1', 'close_1d_ret_hedged_lag1', 'open_close_ret', 'open_close_ret_hedged', 'close_open_ret', 'close_open_ret_hedged', 'high_low_ret', 'high_low_ret_hedged', 'body_ratio']\n",
    "non_linear_features = ['share_dollar_volume', 'turnover_proxy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f4aa4",
   "metadata": {},
   "source": [
    "# Feature list\n",
    "1. Avg past returns close to close\n",
    "2. Avg past returns open to close\n",
    "3. Avg past returns low to close\n",
    "4. Avg past returns high to low\n",
    "\n",
    "- Hedged/Not hedged\n",
    "- Clipped/Not clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1bae4",
   "metadata": {},
   "source": [
    "# Models list\n",
    "\n",
    "## Baseline\n",
    "1. Random\n",
    "2. Past returns (define period)\n",
    "3. MACD vol adjusted\n",
    "4. Linear Regression (define features + beta)\n",
    "\n",
    "## LTR\n",
    "1. LambdaMART (pairwise)\n",
    "2. LambdaRANK (listwise)\n",
    "3. ListMLE (listwise - use LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0282f",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca42bfa",
   "metadata": {},
   "source": [
    "## Random Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274180e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df['random_signal'] = 0.0\n",
    "\n",
    "def assign_random_signals(group):\n",
    "    n = len(group)\n",
    "    if n >= 70:\n",
    "        signals = np.array([1.0] * 35 + [-1.0] * 35 + [0.0] * (n - 70))\n",
    "        np.random.shuffle(signals)\n",
    "        return pd.Series(signals, index=group.index)\n",
    "    else:\n",
    "        return pd.Series(0.0, index=group.index)\n",
    "\n",
    "df['random_signal'] = df.groupby('Date', group_keys=False).apply(assign_random_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_daily_returns = df.groupby('Date').apply(lambda x: (x['random_signal']*x['close_1d_ret']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "(random_daily_returns.cumsum()*1e-4+1).plot(figsize=(12,6), title='Cumulative Random Strategy Return', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b73b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "random_daily_returns.mean()/random_daily_returns.std()*np.sqrt(252)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4be66",
   "metadata": {},
   "source": [
    "## Momentum Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b479a5e",
   "metadata": {},
   "source": [
    "### Simple Momentum Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c1e5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_signal(x):\n",
    "    if x.notna().sum() == 0:\n",
    "        return pd.Series([np.nan]*len(x), index=x.index)\n",
    "    ranks = x.rank(method='first')\n",
    "    binned = pd.cut(ranks, bins=10, labels=False, include_lowest=True)\n",
    "    signal = binned.map(lambda y: 1 if y == 0 else (-1 if y == 9 else 0))\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['momentum_hedged_signal'] = df.groupby('Date')['close_1d_ret_hedged'].transform(compute_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['momentum_hedged_signal'] = df.groupby('Ticker')['momentum_hedged_signal'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['momentum_signal'] = df.groupby('Date')['close_1d_ret'].transform(compute_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['momentum_signal'] = df.groupby('Ticker')['momentum_signal'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_daily_returns = df.groupby('Date').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'hedged': (x['momentum_hedged_signal'] * x['close_1d_ret']).mean(),\n",
    "        'unhedged': (x['momentum_signal'] * x['close_1d_ret']).mean()\n",
    "    })\n",
    ")*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843059f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(momentum_daily_returns['hedged'].cumsum() + 1, label='Hedged Momentum Strategy')\n",
    "plt.plot(momentum_daily_returns['unhedged'].cumsum() + 1, label='Unhedged Momentum Strategy')\n",
    "\n",
    "plt.title('Cumulative Momentum Strategy Returns')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_daily_returns.mean()/momentum_daily_returns.std()*np.sqrt(252)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91368f",
   "metadata": {},
   "source": [
    "### MACD Vol adjusted Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_span_list = [8, 16, 32]\n",
    "long_span_list = [24, 48, 96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb37d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "for i, (short_span, long_span) in enumerate(zip(short_span_list, long_span_list)):\n",
    "    macd_adj_series = (df.groupby('Ticker')['close_1d_ret'].ewm(span=short_span, adjust=False).mean() - df.groupby('Ticker')['close_1d_ret'].ewm(span=long_span, adjust=False).mean())/df.groupby('Ticker')['close_1d_ret'].rolling(63).std()\n",
    "    macd_adj_series.name = f'macd_adj_{i+1}'\n",
    "    df = df.merge(macd_adj_series, on=['Date', 'Ticker'], how='right')\n",
    "    df[f'macd_adj_{i+1}'] /= df[f'macd_adj_{i+1}'].rolling(252).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e9b64",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def phi_baz(x):\n",
    "    return x / np.sqrt(1 + x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['macd_baz_signal'] = df[['macd_adj_1', 'macd_adj_2', 'macd_adj_3']].apply(phi_baz).sum(1).replace(0, np.nan)\n",
    "df['macd_baz_signal'] = df.groupby('Date')['macd_baz_signal'].transform(compute_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['macd_tanh_signal'] = df[['macd_adj_1', 'macd_adj_2', 'macd_adj_3']].apply(np.tanh).sum(1).replace(0, np.nan)\n",
    "df['macd_tanh_signal']  = df.groupby('Date')['macd_tanh_signal'].transform(compute_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93376a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['macd_baz_signal'] = df.groupby('Ticker')['macd_baz_signal'].shift()\n",
    "df['macd_tanh_signal'] = df.groupby('Ticker')['macd_tanh_signal'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_daily_returns = df.groupby('Date').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'baz': (x['macd_baz_signal'] * x['close_1d_ret']).mean(),\n",
    "        'tanh': (x['macd_tanh_signal'] * x['close_1d_ret']).mean()\n",
    "    })\n",
    ")*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(macd_daily_returns['baz'].cumsum() + 1, label='MACD Strategy - Baz function')\n",
    "plt.plot(macd_daily_returns['tanh'].cumsum() + 1, label='MACD Strategy - Tanh function')\n",
    "\n",
    "plt.title('Cumulative Momentum Strategy Returns')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175110b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "macd_daily_returns.mean()/macd_daily_returns.std()*np.sqrt(252)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60cec5",
   "metadata": {},
   "source": [
    "# Regress-then-rank Strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cae987",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b2b58",
   "metadata": {},
   "source": [
    "### Feature factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee57ee1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def daily_metrics(group, feature, target, beta_global):\n",
    "    reg = LinearRegression(fit_intercept=False)\n",
    "    if len(group) < 2:\n",
    "        return pd.Series({'beta': np.nan, 'bias': np.nan})\n",
    "    reg.fit(group[[feature]], group[target])\n",
    "    predictions = reg.predict(group[[feature]])\n",
    "    bias = np.std(predictions)\n",
    "    if reg.coef_[0]*beta_global < 0:\n",
    "        bias *= -1\n",
    "    return pd.Series({'bias': bias})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd005e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def single_feature_metrics(df, feature, target, drop_extreme_perc=True, ts_bool=False, fit_intercept=False):\n",
    "    global_reg = LinearRegression(fit_intercept=fit_intercept)\n",
    "    if drop_extreme_perc:\n",
    "        first_perc, last_perc = np.percentile(df[feature].dropna(), [1, 99])\n",
    "        df_filtered = df[(df[feature]>=first_perc) & (df[feature]<=last_perc)]\n",
    "    else:\n",
    "        df_filtered = df.copy(deep=True)\n",
    "    global_reg.fit(df_filtered[[feature]], df_filtered[target])\n",
    "    predictions = global_reg.predict(df_filtered[[feature]])\n",
    "    bias = np.std(predictions)\n",
    "    stab = (predictions * df_filtered[target] > 0).mean() * 100\n",
    "    beta_global = global_reg.coef_[0]\n",
    "    if ts_bool:\n",
    "        bias_ts = df_filtered.groupby('Date').apply(lambda x: daily_metrics(x, feature, target, beta_global))\n",
    "        mean_bias_ts = bias_ts['bias'].mean()\n",
    "        sharpe_ts = bias_ts['bias'].mean() / bias_ts['bias'].std() * np.sqrt(252)\n",
    "        return bias, mean_bias_ts, stab, beta_global, sharpe_ts, bias_ts\n",
    "    else:\n",
    "        return bias, stab, beta_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa6956",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50201926",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_linear_features = copy.deepcopy(linear_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e19bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in linear_features:\n",
    "    print(feature)\n",
    "    plt.hist(pd.Series(winsorize(df[feature], limits=0.001)).dropna(), bins=100)\n",
    "    plt.title(f'{feature} hist')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in tqdm(linear_features):\n",
    "    for h in [3, 5, 10, 15, 20, 30, 60, 80]:\n",
    "        df[f'avg_{feature}_{h}d'] = df.groupby('Ticker')[feature].transform(lambda x: x.rolling(h).mean())\n",
    "        df[f'zscore_{feature}_{h}d'] = (df[feature]-df[f'avg_{feature}_{h}d'])/df.groupby('Ticker')[feature].transform(lambda x: x.rolling(h).std())\n",
    "        df[f'sharpe_{feature}_{h}d'] = df[f'avg_{feature}_{h}d']/df.groupby('Ticker')[feature].transform(lambda x: x.rolling(h).std())\n",
    "        all_linear_features.append(f'avg_{feature}_{h}d')\n",
    "        all_linear_features.append(f'zscore_{feature}_{h}d')\n",
    "        all_linear_features.append(f'sharpe_{feature}_{h}d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8879c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('df.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in tqdm(all_linear_features):\n",
    "    df[feature] = winsorize(df[feature], limits=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2279ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in linear_features:\n",
    "#     print(feature)\n",
    "#     plt.hist(df[feature].replace([-np.inf, np.inf], np.nan), bins=100)\n",
    "#     plt.title(f'{feature} hist')\n",
    "#     plt.show()\n",
    "#     for feature_type in ['avg', 'zscore', 'sharpe']:\n",
    "#         for window in [5, 30, 80]:\n",
    "#             temp_feature = f'{feature_type}_{feature}_{window}d'\n",
    "#             plt.hist(df[temp_feature].replace([-np.inf, np.inf], np.nan), bins=100)\n",
    "#             plt.title(f'{temp_feature} hist')\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = {'feature': [], 'bias': [], 'mean_bias_ts': [], 'stability': [], 'beta': [], 'sharpe_ts': [], 'bias_ts': []}\n",
    "for feature in tqdm(all_linear_features):\n",
    "    bias, mean_bias_ts, stability, beta, sharpe_ts, bias_ts = single_feature_metrics(df[[feature, 'close_1d_ret']].dropna(), feature, 'close_1d_ret', drop_extreme_perc=False,  ts_bool=True)\n",
    "    dict_results['feature'].append(feature)\n",
    "    dict_results['bias'].append(bias)\n",
    "    dict_results['mean_bias_ts'].append(mean_bias_ts)\n",
    "    dict_results['stability'].append(stability)\n",
    "    dict_results['beta'].append(beta)\n",
    "    dict_results['sharpe_ts'].append(sharpe_ts)\n",
    "    dict_results['bias_ts'].append(bias_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"dict_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(dict_results, f)\n",
    "\n",
    "with open(\"dict_results.pkl\", \"rb\") as f:\n",
    "    dict_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(dict_results).set_index('feature')\n",
    "results_df['feature_type'] = pd.Series(results_df.index.str.split('_')).apply(lambda x: x[0] if x[0] in ['avg', 'zscore', 'sharpe'] else 'spot').values\n",
    "results_df['window'] = pd.Series(results_df.index.str.split('_')).apply(lambda x: int(x[-1][:-1]) if (x[-1][-1]=='d' and x[-1]!='hedged') else 1).values\n",
    "results_df['feature_name'] = np.where(results_df['feature_type']!='spot', pd.Series(results_df.index.str.split('_')).apply(lambda x: '_'.join(x[1:-1])), results_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc98ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('mean_bias_ts').tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc['zscore_open_close_ret_60d', 'bias_ts'].cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad2d36",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d118544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25178ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('df.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6814688",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "X = df[all_linear_features + [\"close_1d_ret\"]].replace([-np.inf, np.inf], np.nan).dropna()[all_linear_features]\n",
    "y = df[all_linear_features + [\"close_1d_ret\"]].replace([-np.inf, np.inf], np.nan).dropna()[\"close_1d_ret\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Expanding CV function\n",
    "# ---------------------------\n",
    "def expanding_cv_score(model, X, y, n_splits=5, lower=0.01, upper=0.01):\n",
    "    \"\"\"\n",
    "    Performs expanding-window CV with winsorization applied to X.\n",
    "    Winsorization limits are fit on each training window and applied to validation window.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: estimator with fit/predict\n",
    "    - X: pandas DataFrame\n",
    "    - y: pandas Series\n",
    "    - n_splits: number of expanding CV splits\n",
    "    - lower, upper: winsorization proportions\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    split_size = (n_samples // 2) // n_splits  # size of each incremental step\n",
    "\n",
    "    mse_list = []\n",
    "\n",
    "    for i in tqdm(range(n_splits)):\n",
    "        train_end = n_samples//2 + split_size * i\n",
    "        val_end = n_samples//2 + split_size * (i + 1)\n",
    "\n",
    "        # Slice windows\n",
    "        X_train_cv = X.iloc[:train_end].copy()\n",
    "        y_train_cv = y.iloc[:train_end]\n",
    "\n",
    "        X_val_cv = X.iloc[train_end:val_end].copy()\n",
    "        y_val_cv = y.iloc[train_end:val_end]\n",
    "\n",
    "        # ---- Winsorization ----\n",
    "        # Fit limits on TRAINING data only\n",
    "        lower_bounds = {}\n",
    "        upper_bounds = {}\n",
    "\n",
    "        for col in X.columns:\n",
    "            lb = np.percentile(X_train_cv[col], lower * 100)\n",
    "            ub = np.percentile(X_train_cv[col], 100 - upper * 100)\n",
    "\n",
    "            lower_bounds[col] = lb\n",
    "            upper_bounds[col] = ub\n",
    "\n",
    "            # apply to train\n",
    "            X_train_cv[col] = np.clip(X_train_cv[col], lb, ub)\n",
    "            # apply SAME limits to val\n",
    "            X_val_cv[col] = np.clip(X_val_cv[col], lb, ub)\n",
    "\n",
    "        # Fit + evaluate model\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_cv = model.predict(X_val_cv)\n",
    "        mse_list.append(mean_squared_error(y_val_cv, y_pred_cv))\n",
    "\n",
    "    return mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Hyperparameter tuning\n",
    "# ---------------------------\n",
    "alphas = [0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 100.0]\n",
    "alpha_mse = {}\n",
    "\n",
    "for a in tqdm(alphas):\n",
    "    ridge = Ridge(alpha=a, fit_intercept=False)\n",
    "    mse = expanding_cv_score(ridge, X_train, y_train, n_splits=5)\n",
    "    alpha_mse[a] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983df8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Final model training on full training set\n",
    "# ---------------------------\n",
    "ridge_reg = Ridge(alpha=best_alpha, fit_intercept=False)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ridge_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nRidge Coefficients:\", ridge_reg.coef_)\n",
    "print(\"Ridge Intercept:\", ridge_reg.intercept_)\n",
    "print(\"Test MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da755726",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in [3, 5, 10, 20, 50, 100]:\n",
    "    for clip in [100, 200, 300, 400]:\n",
    "        df[f'avg_ret_hedged_{h}d_clip{clip}'] = df[f'avg_ret_hedged_{h}d'].clip(-clip, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for h in dict_results['avg_ret_hedged'].keys():\n",
    "    dict_results['avg_ret_hedged'][h][-1]['bias'].cumsum().plot(label=f'{h} days')\n",
    "plt.legend()\n",
    "plt.title('Cumulative Bias Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Bias')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3d647",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4de0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e326386",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fed623",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912679cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
